{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#####################################\n",
        "# AutoGen Llama 3 Text to SQL Evaluation\n",
        "pip install pyautogen spider-env\n",
        "########################################\n",
        "#Using Groq\n",
        "export GROQ_API_KEY=xxxxxxxxxxxx\n",
        "################################################"
      ],
      "metadata": {
        "id": "x-AGcGnOUY0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIN7hEOKUPeh"
      },
      "outputs": [],
      "source": [
        "################################################\n",
        "# 1. Configuration\n",
        "import json\n",
        "import os\n",
        "from typing import Annotated, Dict\n",
        "from spider_env import SpiderEnv\n",
        "from autogen import ConversableAgent, UserProxyAgent, config_list_from_json\n",
        "\n",
        "os.environ[\"AUTOGEN_USE_DOCKER\"] = \"False\"\n",
        "llm_config = {\n",
        "    \"cache_seed\": 48,\n",
        "    \"config_list\": [{\n",
        "        \"model\": os.environ.get(\"OPENAI_MODEL_NAME\", \"llama3-70b-8192\"),\n",
        "        \"api_key\": os.environ[\"GROQ_API_KEY\"],\n",
        "        \"base_url\": os.environ.get(\"OPENAI_API_BASE\", \"https://api.groq.com/openai/v1\")}\n",
        "    ],\n",
        "}\n",
        "\n",
        "# 2. Import Data\n",
        "gym = SpiderEnv()\n",
        "observation, info = gym.reset()\n",
        "question = observation[\"instruction\"]\n",
        "print(question)\n",
        "schema = info[\"schema\"]\n",
        "print(schema)\n",
        "\n",
        "# 3. Create Agents\n",
        "def check_termination(msg: Dict):\n",
        "    if \"tool_responses\" not in msg:\n",
        "        return False\n",
        "    json_str = msg[\"tool_responses\"][0][\"content\"]\n",
        "    obj = json.loads(json_str)\n",
        "    return \"error\" not in obj or obj[\"error\"] is None and obj[\"reward\"] == 1\n",
        "\n",
        "sql_writer = ConversableAgent(\n",
        "    \"sql_writer\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"You are good at writing SQL queries. Always respond with a function call to execute_sql().\",\n",
        "    is_termination_msg=check_termination,\n",
        ")\n",
        "\n",
        "user_proxy = UserProxyAgent(\n",
        "    \"user_proxy\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=5\n",
        ")\n",
        "\n",
        "# 4. Create Tools / Function Calling\n",
        "@sql_writer.register_for_llm(description=\"Function for executing SQL query and returning a response\")\n",
        "@user_proxy.register_for_execution()\n",
        "def execute_sql(reflection: Annotated[str, \"Think about what to do\"], sql: Annotated[str, \"SQL query\"]) -> Annotated[Dict[str, str], \"Dictionary with keys 'result' and 'error'\"]:\n",
        "    observation, reward, _, _, info = gym.step(sql)\n",
        "    error = observation[\"feedback\"][\"error\"]\n",
        "    if not error and reward == 0:\n",
        "        error = \"The SQL query returned an incorrect result\"\n",
        "    if error:\n",
        "        return { \"error\": error, \"wrong_result\": observation[\"feedback\"][\"result\"], \"correct_result\": info[\"gold_result\"], }\n",
        "    else:\n",
        "        return { \"result\": observation[\"feedback\"][\"result\"], }\n",
        "\n",
        "\n",
        "# 5. Initiate Chat\n",
        "prompt_template = f\"\"\"Below is the schema for a SQL database:\n",
        "{schema}\n",
        "Generate a SQL query to answer the following question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "user_proxy.initiate_chat(sql_writer, message=prompt_template)\n",
        "\n",
        "#############################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#############################################################\n",
        "\n",
        "#Using Ollama\n",
        "export OPENAI_API_BASE=http://localhost:11434/v1\n",
        "\n",
        "export OPENAI_MODEL_NAME=llama3"
      ],
      "metadata": {
        "id": "RZn_gfYVUqU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qijc_lajUVNv"
      }
    }
  ]
}